/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    NMBU Orion HPC â€” site-specific configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Activated by:  -profile orion
    Cluster: 6 nodes (cn-31..cn-35, cn-37), 384 CPUs, ~1.5 TB RAM each (shared)
    Filesystems: IBM Storage Scale (GPFS) exported via NFS4, 3.5 TB local SSD per node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

// --- Orion-specific database paths (override on CLI for other sites) ---
params {
    mmseqs2_swissprot   = '/mnt/project/glowberry/transannot/db/SwissProtDB'
    mmseqs2_pfam        = '/mnt/project/glowberry/transannot/db/PfamDB'
    mmseqs2_eggnog      = '/mnt/project/glowberry/transannot/db/eggNOG7DB'
    mmseqs2_taxonomy_db = '/mnt/project/FjellheimLab/martpali/AnnualPerennial/nf-denovoslim/db/UniRef50taxdb'
    eggnog_annotations  = '/mnt/project/glowberry/transannot/db/e7_as_e5_annotations.tsv'
    busco_lineage       = 'poales_odb12'

    // Orion nodes: 384 CPUs, ~1.5 TB RAM each
    max_cpus            = 64
    max_memory          = 1200.GB
    max_time            = 336.h   // 14 days
}

process {
    executor       = 'slurm'
    queue          = 'orion'

    // Exclude nodes with nearly-full local disks (check with cn_quota.sh)
    clusterOptions = { params.orion_exclude_nodes ? "--exclude=${params.orion_exclude_nodes}" : '' }

    // Use node-local SSD via $TMPDIR for all task I/O.
    // $TMPDIR is a per-job directory on compute nodes (SLURM creates it and
    // cleans it automatically when the job ends).  Nextflow creates unique
    // nxf-XXXXXX temp dirs underneath, stages inputs, runs the task,
    // rsync's outputs back to the NFS work dir, and removes the temp dir.
    // NOTE: On login nodes $TMPDIR is still /work/users/$USER (persistent).
    scratch        = '\$TMPDIR'

    // Group ownership fix (three layers):
    //   1. sbatch wrappers run `sg <group>` so the Nextflow HEAD process
    //      creates work-dir scaffolding under the project group.
    //   2. beforeScript sets setgid+umask on the scratch dir so all task
    //      outputs inherit the project group.
    //   3. beforeScript also sets setgid on the NFS work dir so that
    //      outputs rsync'd back from scratch inherit the correct group.
    beforeScript   = {
        params.unix_group ? """
            chgrp ${params.unix_group} . 2>/dev/null || true
            chmod g+s . 2>/dev/null || true
            umask 002
            if [ -n "\${NXF_TASK_WORKDIR:-}" ] && [ "\$NXF_TASK_WORKDIR" != "\$PWD" ]; then
                chgrp ${params.unix_group} "\$NXF_TASK_WORKDIR" 2>/dev/null || true
                chmod g+s "\$NXF_TASK_WORKDIR" 2>/dev/null || true
            fi
        """ : ''
    }

    // Safety net: catch files that escaped the setgid fix
    afterScript    = {
        params.unix_group ? """
            chgrp -R ${params.unix_group} . 2>/dev/null || true
            chmod -R g+rwX . 2>/dev/null || true
        """ : ''
    }
}

executor {
    submitRateLimit = '30/1min'
    queueSize       = 20
}

// Bind-mount Orion filesystems into containers
apptainer.runOptions   = '-B /mnt/project:/mnt/project -B /net/fs-2/scale:/net/fs-2/scale -B /work:/work'
singularity.runOptions = '-B /mnt/project:/mnt/project -B /net/fs-2/scale:/net/fs-2/scale -B /work:/work'
